{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846a900e",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline\n",
    "\n",
    "## Overview\n",
    "This preprocessing pipeline transforms raw insurance data into ML-ready features with **empirically-derived risk scores** based on actual claim patterns observed in the data.\n",
    "\n",
    "## Key Principles\n",
    "- **Data-Driven**: Risk scores calculated from actual claim rates (not assumptions)\n",
    "- **Validated**: Claims consistently show higher risk scores than no-claims (+8.15%)\n",
    "- **Stratified**: Test/validation sets preserve real-world distribution (6.4% claims)\n",
    "- **Balanced Training**: Undersampled to 20% claims for better model learning\n",
    "\n",
    "\n",
    "\n",
    "## Pipeline Steps\n",
    "1. **Load Data** - Import cleaned dataset (58,592 policies)\n",
    "2. **Feature Engineering** - Create empirical risk scores from observed patterns\n",
    "3. **Validation** - Verify risk scores align with actual outcomes\n",
    "4. **Stratified Split** - Create train/val/test sets (70/15/15)\n",
    "5. **Balance Training** - Undersample majority class to 20% claim rate\n",
    "6. **Save Outputs** - Export processed datasets\n",
    "\n",
    "## Output Files\n",
    "- `train_balanced.csv` - 13,120 records (20% claims) for training\n",
    "- `validation.csv` - 8,791 records (6.4% claims) for tuning\n",
    "- `test.csv` - 8,789 records (6.4% claims) for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d9cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b6672",
   "metadata": {},
   "source": [
    "### LOAD CLEANED DATA\n",
    "\n",
    "\n",
    "- **Purpose:** Load the cleaned insurance dataset and verify data integrity\n",
    "- **Input:** data/processed/cleaned_data.csv\n",
    "- **Output:** DataFrame with 58,592 policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a7fd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: LOADING CLEANED DATA\n",
      "======================================================================\n",
      "‚úì Loaded 58,592 records with 41 columns\n",
      "‚úì Claim rate: 6.40%\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: LOADING CLEANED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_csv('../data/processed/cleaned_data.csv')\n",
    "print(f\"‚úì Loaded {len(df):,} records with {len(df.columns)} columns\")\n",
    "print(f\"‚úì Claim rate: {(df['claim_status']==1).mean()*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6cd5a048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>subscription_length</th>\n",
       "      <th>vehicle_age</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>region_code</th>\n",
       "      <th>region_density</th>\n",
       "      <th>segment</th>\n",
       "      <th>model</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>max_torque</th>\n",
       "      <th>...</th>\n",
       "      <th>is_brake_assist</th>\n",
       "      <th>is_power_door_locks</th>\n",
       "      <th>is_central_locking</th>\n",
       "      <th>is_power_steering</th>\n",
       "      <th>is_driver_seat_height_adjustable</th>\n",
       "      <th>is_day_night_rear_view_mirror</th>\n",
       "      <th>is_ecw</th>\n",
       "      <th>is_speed_alert</th>\n",
       "      <th>ncap_rating</th>\n",
       "      <th>claim_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POL045360</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>41</td>\n",
       "      <td>C8</td>\n",
       "      <td>8794</td>\n",
       "      <td>C2</td>\n",
       "      <td>M4</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>250Nm@2750rpm</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL016745</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>35</td>\n",
       "      <td>C2</td>\n",
       "      <td>27003</td>\n",
       "      <td>C1</td>\n",
       "      <td>M9</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>200Nm@1750rpm</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POL007194</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>44</td>\n",
       "      <td>C8</td>\n",
       "      <td>8794</td>\n",
       "      <td>C2</td>\n",
       "      <td>M4</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>250Nm@2750rpm</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POL018146</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>44</td>\n",
       "      <td>C10</td>\n",
       "      <td>73430</td>\n",
       "      <td>A</td>\n",
       "      <td>M1</td>\n",
       "      <td>CNG</td>\n",
       "      <td>60Nm@3500rpm</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POL049011</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>C13</td>\n",
       "      <td>5410</td>\n",
       "      <td>B2</td>\n",
       "      <td>M5</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>200Nm@3000rpm</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   policy_id  subscription_length  vehicle_age  customer_age region_code  \\\n",
       "0  POL045360                  9.3          1.2            41          C8   \n",
       "1  POL016745                  8.2          1.8            35          C2   \n",
       "2  POL007194                  9.5          0.2            44          C8   \n",
       "3  POL018146                  5.2          0.4            44         C10   \n",
       "4  POL049011                 10.1          1.0            56         C13   \n",
       "\n",
       "   region_density segment model fuel_type     max_torque  ... is_brake_assist  \\\n",
       "0            8794      C2    M4    Diesel  250Nm@2750rpm  ...               1   \n",
       "1           27003      C1    M9    Diesel  200Nm@1750rpm  ...               0   \n",
       "2            8794      C2    M4    Diesel  250Nm@2750rpm  ...               1   \n",
       "3           73430       A    M1       CNG   60Nm@3500rpm  ...               0   \n",
       "4            5410      B2    M5    Diesel  200Nm@3000rpm  ...               0   \n",
       "\n",
       "  is_power_door_locks  is_central_locking  is_power_steering  \\\n",
       "0                   1                   1                  1   \n",
       "1                   1                   1                  1   \n",
       "2                   1                   1                  1   \n",
       "3                   0                   0                  1   \n",
       "4                   1                   1                  1   \n",
       "\n",
       "   is_driver_seat_height_adjustable  is_day_night_rear_view_mirror  is_ecw  \\\n",
       "0                                 1                              0       1   \n",
       "1                                 1                              1       1   \n",
       "2                                 1                              0       1   \n",
       "3                                 0                              0       0   \n",
       "4                                 0                              0       1   \n",
       "\n",
       "   is_speed_alert ncap_rating  claim_status  \n",
       "0               1           3             0  \n",
       "1               1           4             0  \n",
       "2               1           3             0  \n",
       "3               1           0             0  \n",
       "4               1           5             0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1742b47",
   "metadata": {},
   "source": [
    "### FEATURE ENGINEERING\n",
    "\n",
    "- **Purpose:** Create composite risk scores and categorical bins\n",
    "- **Why:** Enriches text summaries with meaningful risk context\n",
    "- **Output:** 6 risk scores + 4 categorical groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f151b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: DATA-DRIVEN FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "üìä Creating empirical risk scores based on ACTUAL claim patterns...\n",
      "‚úì Created 5 empirical risk scores\n",
      "\n",
      "üìä Calculating feature importance weights...\n",
      "\n",
      "   Feature weights (based on correlation with claims):\n",
      "      subscription: 0.507 (corr: 0.0808)\n",
      "      driver      : 0.143 (corr: 0.0227)\n",
      "      region      : 0.139 (corr: 0.0222)\n",
      "      vehicle     : 0.123 (corr: 0.0195)\n",
      "      safety      : 0.088 (corr: 0.0141)\n",
      "\n",
      "‚úì Overall risk score range: 0.000 to 1.000\n",
      "\n",
      "üìä Risk category distribution:\n",
      "risk_category\n",
      "LOW           4470\n",
      "MODERATE     18636\n",
      "HIGH         17306\n",
      "VERY HIGH    18180\n",
      "Name: count, dtype: int64\n",
      "‚úì Created categorical groupings for text generation context\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================================================================\n",
    "# STEP 2: DATA-DRIVEN FEATURE ENGINEERING\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: DATA-DRIVEN FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calculate_empirical_risk_score(feature_col, target_col, n_bins=5):\n",
    "    \"\"\"\n",
    "    Calculate risk score based on ACTUAL claim rates observed in the data.\n",
    "    This ensures risk scores reflect reality, not assumptions.\n",
    "    \n",
    "    Args:\n",
    "        feature_col: The feature to bin and analyze\n",
    "        target_col: The target variable (claim_status)\n",
    "        n_bins: Number of bins to create\n",
    "    \n",
    "    Returns:\n",
    "        Normalized risk score (0-1) where higher = higher observed claim rate\n",
    "    \"\"\"\n",
    "    # Create bins (quantile-based for even distribution)\n",
    "    try:\n",
    "        feature_binned = pd.qcut(feature_col, q=n_bins, duplicates='drop')\n",
    "    except:\n",
    "        # If qcut fails (e.g., too few unique values), use regular cut\n",
    "        feature_binned = pd.cut(feature_col, bins=n_bins)\n",
    "    \n",
    "    # Create a temporary dataframe to calculate claim rates per bin\n",
    "    temp_df = pd.DataFrame({\n",
    "        'bin': feature_binned,\n",
    "        'target': target_col\n",
    "    })\n",
    "    \n",
    "    # Calculate actual claim rate in each bin\n",
    "    bin_claim_rates = temp_df.groupby('bin', observed=True)['target'].mean()\n",
    "    \n",
    "    # Map claim rates back to original data (convert to numeric)\n",
    "    risk_scores = feature_binned.map(bin_claim_rates).astype(float)\n",
    "    \n",
    "    # Normalize to 0-1 scale\n",
    "    min_rate = risk_scores.min()\n",
    "    max_rate = risk_scores.max()\n",
    "    \n",
    "    if max_rate > min_rate:\n",
    "        normalized_scores = (risk_scores - min_rate) / (max_rate - min_rate)\n",
    "    else:\n",
    "        # If all bins have same rate, return middle value\n",
    "        normalized_scores = pd.Series(0.5, index=risk_scores.index)\n",
    "    \n",
    "    return normalized_scores\n",
    "\n",
    "print(\"\\nüìä Creating empirical risk scores based on ACTUAL claim patterns...\")\n",
    "\n",
    "# 2.1 Customer Age Risk (based on YOUR EDA showing 56+ has 7.54% claims)\n",
    "df['driver_risk_score'] = calculate_empirical_risk_score(\n",
    "    df['customer_age'], \n",
    "    df['claim_status'], \n",
    "    n_bins=5\n",
    ")\n",
    "\n",
    "# 2.2 Vehicle Age Risk (based on YOUR EDA showing 0-3yrs has 6.12% claims)\n",
    "df['vehicle_risk_score'] = calculate_empirical_risk_score(\n",
    "    df['vehicle_age'], \n",
    "    df['claim_status'], \n",
    "    n_bins=3\n",
    ")\n",
    "\n",
    "# 2.3 Subscription Length Risk (YOUR HIGHEST CORRELATION: 0.078738)\n",
    "df['subscription_risk_score'] = calculate_empirical_risk_score(\n",
    "    df['subscription_length'], \n",
    "    df['claim_status'], \n",
    "    n_bins=5\n",
    ")\n",
    "\n",
    "# 2.4 Region Density Risk\n",
    "df['region_risk_score'] = calculate_empirical_risk_score(\n",
    "    df['region_density'], \n",
    "    df['claim_status'], \n",
    "    n_bins=5\n",
    ")\n",
    "\n",
    "# 2.5 Safety Features Risk (composite of all safety features)\n",
    "# First create a safety composite score\n",
    "df['safety_composite'] = (\n",
    "    df['airbags']/6 + \n",
    "    df['is_esc'] + \n",
    "    df['is_brake_assist'] + \n",
    "    df['is_parking_sensors'] + \n",
    "    df['is_tpms'] + \n",
    "    df['ncap_rating']/5\n",
    ") / 6\n",
    "\n",
    "df['safety_score'] = calculate_empirical_risk_score(\n",
    "    df['safety_composite'], \n",
    "    df['claim_status'], \n",
    "    n_bins=5\n",
    ")\n",
    "\n",
    "print(f\"‚úì Created 5 empirical risk scores\")\n",
    "\n",
    "# 2.6 Calculate correlation-based weights\n",
    "print(f\"\\nüìä Calculating feature importance weights...\")\n",
    "\n",
    "correlations = {\n",
    "    'subscription': abs(df['subscription_risk_score'].corr(df['claim_status'])),\n",
    "    'driver': abs(df['driver_risk_score'].corr(df['claim_status'])),\n",
    "    'vehicle': abs(df['vehicle_risk_score'].corr(df['claim_status'])),\n",
    "    'region': abs(df['region_risk_score'].corr(df['claim_status'])),\n",
    "    'safety': abs(df['safety_score'].corr(df['claim_status']))\n",
    "}\n",
    "\n",
    "# Normalize weights to sum to 1\n",
    "total_corr = sum(correlations.values())\n",
    "weights = {k: v/total_corr for k, v in correlations.items()}\n",
    "\n",
    "print(f\"\\n   Feature weights (based on correlation with claims):\")\n",
    "for feature, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"      {feature:12s}: {weight:.3f} (corr: {correlations[feature]:.4f})\")\n",
    "\n",
    "# 2.7 Create weighted overall risk score\n",
    "df['overall_risk_score'] = (\n",
    "    weights['subscription'] * df['subscription_risk_score'] +\n",
    "    weights['driver'] * df['driver_risk_score'] +\n",
    "    weights['vehicle'] * df['vehicle_risk_score'] +\n",
    "    weights['region'] * df['region_risk_score'] +\n",
    "    weights['safety'] * df['safety_score']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Overall risk score range: {df['overall_risk_score'].min():.3f} to {df['overall_risk_score'].max():.3f}\")\n",
    "\n",
    "# 2.8 Create risk categories\n",
    "df['risk_category'] = pd.cut(\n",
    "    df['overall_risk_score'],\n",
    "    bins=[0, 0.25, 0.5, 0.75, 1.0],\n",
    "    labels=['LOW', 'MODERATE', 'HIGH', 'VERY HIGH'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Risk category distribution:\")\n",
    "print(df['risk_category'].value_counts().sort_index())\n",
    "\n",
    "# 2.9 Create contextual categorical features (for text generation)\n",
    "df['age_group'] = pd.cut(\n",
    "    df['customer_age'],\n",
    "    bins=[0, 35, 45, 55, 100],\n",
    "    labels=['young', 'middle_aged', 'mature', 'senior']\n",
    ")\n",
    "\n",
    "df['vehicle_age_group'] = pd.cut(\n",
    "    df['vehicle_age'],\n",
    "    bins=[0, 3, 7, 100],\n",
    "    labels=['new', 'moderate', 'old']\n",
    ")\n",
    "\n",
    "df['subscription_category'] = pd.cut(\n",
    "    df['subscription_length'],\n",
    "    bins=[0, 3, 6, 9, 100],\n",
    "    labels=['very_short', 'short', 'medium', 'long']\n",
    ")\n",
    "\n",
    "print(f\"‚úì Created categorical groupings for text generation context\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2edb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: VALIDATING RISK SCORES\n",
      "======================================================================\n",
      "\n",
      "‚úÖ OVERALL RISK SCORE VALIDATION:\n",
      "   Claims avg risk:     0.6630\n",
      "   No-claims avg risk:  0.5815\n",
      "   Difference:          +0.0815 ‚úÖ CORRECT!\n",
      "\n",
      "üìä Component-wise validation:\n",
      "   subscription_risk_score  : +0.1306 ‚úÖ\n",
      "   driver_risk_score        : +0.0343 ‚úÖ\n",
      "   vehicle_risk_score       : +0.0356 ‚úÖ\n",
      "   region_risk_score        : +0.0284 ‚úÖ\n",
      "   safety_score             : +0.0228 ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 3: CRITICAL VALIDATION - Risk Scores Must Make Sense!\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: VALIDATING RISK SCORES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "claim_mask = df['claim_status'] == 1\n",
    "no_claim_mask = df['claim_status'] == 0\n",
    "\n",
    "print(f\"\\n‚úÖ OVERALL RISK SCORE VALIDATION:\")\n",
    "claim_risk = df[claim_mask]['overall_risk_score'].mean()\n",
    "no_claim_risk = df[no_claim_mask]['overall_risk_score'].mean()\n",
    "difference = claim_risk - no_claim_risk\n",
    "\n",
    "print(f\"   Claims avg risk:     {claim_risk:.4f}\")\n",
    "print(f\"   No-claims avg risk:  {no_claim_risk:.4f}\")\n",
    "print(f\"   Difference:          {difference:+.4f} {'‚úÖ CORRECT!' if difference > 0 else '‚ùå ERROR!'}\")\n",
    "\n",
    "if difference <= 0:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  WARNING: Risk scores are inverted or flat!\")\n",
    "    print(f\"   This means the model won't learn meaningful patterns.\")\n",
    "\n",
    "print(f\"\\nüìä Component-wise validation:\")\n",
    "for score_col in ['subscription_risk_score', 'driver_risk_score', 'vehicle_risk_score', \n",
    "                  'region_risk_score', 'safety_score']:\n",
    "    claim_avg = df[claim_mask][score_col].mean()\n",
    "    no_claim_avg = df[no_claim_mask][score_col].mean()\n",
    "    diff = claim_avg - no_claim_avg\n",
    "    status = '‚úÖ' if diff > 0 else '‚ö†Ô∏è'\n",
    "    print(f\"   {score_col:25s}: {diff:+.4f} {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d5a1b",
   "metadata": {},
   "source": [
    "### STRATIFIED DATA SPLITTING\n",
    "- **Purpose:** Split data while preserving class distribution\n",
    "- **Strategy:** 70% train / 15% validation / 15% test\n",
    "- **Why:** Prevents data leakage and ensures honest evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7900e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: STRATIFIED DATA SPLITTING\n",
      "======================================================================\n",
      "‚úì Train set: 41,012 records (6.40% claims)\n",
      "‚úì Val set:   8,791 records (6.39% claims)\n",
      "‚úì Test set:  8,789 records (6.39% claims)\n",
      "\n",
      "üìä Risk score validation across splits:\n",
      "   Train: Claims 0.660 vs No-Claims 0.582 = +0.079 ‚úÖ\n",
      "   Val  : Claims 0.661 vs No-Claims 0.584 = +0.078 ‚úÖ\n",
      "   Test : Claims 0.677 vs No-Claims 0.578 = +0.099 ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================================================================\n",
    "# STEP 4: STRATIFIED DATA SPLITTING\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: STRATIFIED DATA SPLITTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split BEFORE any balancing to maintain realistic test set\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.15,\n",
    "    stratify=df['claim_status'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1765,  # 0.15 of remaining = 0.15 total validation\n",
    "    stratify=train_df['claim_status'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train set: {len(train_df):,} records ({(train_df['claim_status']==1).mean()*100:.2f}% claims)\")\n",
    "print(f\"‚úì Val set:   {len(val_df):,} records ({(val_df['claim_status']==1).mean()*100:.2f}% claims)\")\n",
    "print(f\"‚úì Test set:  {len(test_df):,} records ({(test_df['claim_status']==1).mean()*100:.2f}% claims)\")\n",
    "\n",
    "# Validate splits maintain risk score patterns\n",
    "print(f\"\\nüìä Risk score validation across splits:\")\n",
    "for split_name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    claim_r = split_df[split_df['claim_status']==1]['overall_risk_score'].mean()\n",
    "    no_claim_r = split_df[split_df['claim_status']==0]['overall_risk_score'].mean()\n",
    "    diff = claim_r - no_claim_r\n",
    "    status = '‚úÖ' if diff > 0.01 else '‚ö†Ô∏è'\n",
    "    print(f\"   {split_name:5s}: Claims {claim_r:.3f} vs No-Claims {no_claim_r:.3f} = {diff:+.3f} {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac83e4d",
   "metadata": {},
   "source": [
    "### HANDLING CLASS IMBALANCE FOR RAG\n",
    "\n",
    "- **Purpose:** Balance training data for better retrieval\n",
    "- **Method:** Intelligent duplication stratified by risk category\n",
    "- **Target:** 20% claims (up from 6.4%)\n",
    "- **Why:** RAG needs enough claim examples to retrieve from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f20046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: BALANCING TRAINING DATA (TARGET: 20% CLAIMS)\n",
      "======================================================================\n",
      "\n",
      "Before balancing:\n",
      "   Claims:     2,624 (6.40%)\n",
      "   No Claims:  38,388 (93.60%)\n",
      "\n",
      "After balancing:\n",
      "   Claims:     2,624 (20.00%)\n",
      "   No Claims:  10,496 (80.00%)\n",
      "   Total:      13,120 records\n",
      "\n",
      "‚úÖ Risk score validation after balancing:\n",
      "   Claims:     0.6603\n",
      "   No Claims:  0.5806\n",
      "   Difference: +0.0797 ‚úÖ MAINTAINED\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 5: HANDLE CLASS IMBALANCE - RANDOM UNDERSAMPLING\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: BALANCING TRAINING DATA (TARGET: 20% CLAIMS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "train_majority = train_df[train_df['claim_status'] == 0]\n",
    "train_minority = train_df[train_df['claim_status'] == 1]\n",
    "\n",
    "print(f\"\\nBefore balancing:\")\n",
    "print(f\"   Claims:     {len(train_minority):,} ({len(train_minority)/len(train_df)*100:.2f}%)\")\n",
    "print(f\"   No Claims:  {len(train_majority):,} ({len(train_majority)/len(train_df)*100:.2f}%)\")\n",
    "\n",
    "# Calculate how many no-claim samples we need for 20% claim rate\n",
    "# Formula: minority / (minority + majority_new) = 0.20\n",
    "# Solving: majority_new = minority / 0.20 - minority = minority * 4\n",
    "target_majority_size = int(len(train_minority) * 4)\n",
    "\n",
    "# Randomly undersample majority class\n",
    "train_majority_undersampled = train_majority.sample(\n",
    "    n=target_majority_size, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine back\n",
    "balanced_train_df = pd.concat([\n",
    "    train_majority_undersampled, \n",
    "    train_minority\n",
    "]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nAfter balancing:\")\n",
    "print(f\"   Claims:     {len(balanced_train_df[balanced_train_df['claim_status']==1]):,} \"\n",
    "      f\"({(balanced_train_df['claim_status']==1).mean()*100:.2f}%)\")\n",
    "print(f\"   No Claims:  {len(balanced_train_df[balanced_train_df['claim_status']==0]):,} \"\n",
    "      f\"({(balanced_train_df['claim_status']==0).mean()*100:.2f}%)\")\n",
    "print(f\"   Total:      {len(balanced_train_df):,} records\")\n",
    "\n",
    "# Validate balanced data maintains risk patterns\n",
    "claim_risk_balanced = balanced_train_df[balanced_train_df['claim_status']==1]['overall_risk_score'].mean()\n",
    "no_claim_risk_balanced = balanced_train_df[balanced_train_df['claim_status']==0]['overall_risk_score'].mean()\n",
    "diff_balanced = claim_risk_balanced - no_claim_risk_balanced\n",
    "\n",
    "print(f\"\\n‚úÖ Risk score validation after balancing:\")\n",
    "print(f\"   Claims:     {claim_risk_balanced:.4f}\")\n",
    "print(f\"   No Claims:  {no_claim_risk_balanced:.4f}\")\n",
    "print(f\"   Difference: {diff_balanced:+.4f} {'‚úÖ MAINTAINED' if diff_balanced > 0.01 else '‚ö†Ô∏è LOST'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc503c0",
   "metadata": {},
   "source": [
    "### SAVING PREPROCESSED DATA\n",
    "\n",
    "**Output files:**\n",
    "- train_balanced.csv (for embeddings & FAISS index)\n",
    "- validation.csv (for tuning)\n",
    "- test.csv (final evaluation only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebad2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 6: SAVING PROCESSED DATA\n",
      "======================================================================\n",
      "Saved trained data with no balancing  ./data/processed/train.csv\n",
      "‚úÖ Saved balanced training data:   ../data/processed/train_balanced.csv\n",
      "‚úÖ Saved validation data:          ../data/processed/validation.csv\n",
      "‚úÖ Saved test data:                ../data/processed/test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================================================================\n",
    "# STEP 6: SAVE PROCESSED DATA\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: SAVING PROCESSED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save to processed folder\n",
    "train_df.to_csv('../data/processed/train.csv', index=False)\n",
    "balanced_train_df.to_csv('../data/processed/train_balanced.csv', index=False)\n",
    "val_df.to_csv('../data/processed/validation.csv', index=False)\n",
    "test_df.to_csv('../data/processed/test.csv', index=False)\n",
    "\n",
    "print(f\"Saved trained data with no balancing:  ../data/processed/train.csv\")\n",
    "print(f\"‚úÖ Saved balanced training data:   ../data/processed/train_balanced.csv\")\n",
    "print(f\"‚úÖ Saved validation data:          ../data/processed/validation.csv\")\n",
    "print(f\"‚úÖ Saved test data:                ../data/processed/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c660a2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ PREPROCESSING COMPLETE - READY FOR TEXT GENERATION\n",
      "======================================================================\n",
      "\n",
      "üìä FINAL STATISTICS:\n",
      "   Training:   13,120 records (20.0% claims) - BALANCED\n",
      "   Validation: 8,791 records (6.4% claims) - REALISTIC\n",
      "   Test:       8,789 records (6.4% claims) - REALISTIC\n",
      "\n",
      "üéØ RISK SCORES:\n",
      "   ‚úÖ Based on ACTUAL claim patterns in data\n",
      "   ‚úÖ Claims have HIGHER risk scores than no-claims\n",
      "   ‚úÖ Weights determined by correlation strength\n",
      "   ‚úÖ Validated across all splits\n",
      "\n",
      "üìù FEATURES READY FOR TEXT GENERATION:\n",
      "   ‚úÖ 5 granular risk scores (driver, vehicle, subscription, region, safety)\n",
      "   ‚úÖ 1 overall weighted risk score\n",
      "   ‚úÖ Risk categories (LOW, MODERATE, HIGH, VERY HIGH)\n",
      "   ‚úÖ Age groups, vehicle age groups, subscription categories\n",
      "   ‚úÖ All safety features preserved\n",
      "\n",
      "üöÄ NEXT STEP: Text generation using these validated risk scores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 7: FINAL SUMMARY\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PREPROCESSING COMPLETE - READY FOR TEXT GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä FINAL STATISTICS:\n",
    "   Training:   {len(balanced_train_df):,} records (20.0% claims) - BALANCED\n",
    "   Validation: {len(val_df):,} records ({(val_df['claim_status']==1).mean()*100:.1f}% claims) - REALISTIC\n",
    "   Test:       {len(test_df):,} records ({(test_df['claim_status']==1).mean()*100:.1f}% claims) - REALISTIC\n",
    "\n",
    "üéØ RISK SCORES:\n",
    "   ‚úÖ Based on ACTUAL claim patterns in data\n",
    "   ‚úÖ Claims have HIGHER risk scores than no-claims\n",
    "   ‚úÖ Weights determined by correlation strength\n",
    "   ‚úÖ Validated across all splits\n",
    "\n",
    "üìù FEATURES READY FOR TEXT GENERATION:\n",
    "   ‚úÖ 5 granular risk scores (driver, vehicle, subscription, region, safety)\n",
    "   ‚úÖ 1 overall weighted risk score\n",
    "   ‚úÖ Risk categories (LOW, MODERATE, HIGH, VERY HIGH)\n",
    "   ‚úÖ Age groups, vehicle age groups, subscription categories\n",
    "   ‚úÖ All safety features preserved\n",
    "\n",
    "üöÄ NEXT STEP: Text generation using these validated risk scores\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
